{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75aadd8",
   "metadata": {},
   "source": [
    "# Model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d6dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NamedTuple\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import func_to_container_op\n",
    "from kubernetes.client.models impoort V1ContainerPort\n",
    "from kfp.components import InputPath, OutputPath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451af5c5",
   "metadata": {},
   "source": [
    "# Adjust the number of Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_CLIENTS = 2 #Custom number of clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718d8c8",
   "metadata": {},
   "source": [
    "# Import necessary libraries and modules:\n",
    "\n",
    "Flask: Used for building web server applications.\n",
    "Other Python standard libraries and third-party libraries, such as NumPy, Pandas, and TensorFlow, are used for data processing, machine learning, etc.\n",
    "threading: used to establish multiple threads to achieve synchronous and asynchronous operations.\n",
    "\n",
    "# Define some global variables and locks:\n",
    "\n",
    "global_value: A dictionary that stores some shared global variables, such as execution status, data status, global count, average weight, etc.\n",
    "Different locks are used for different purposes, such as initialization locks, client local count locks, scaling local weight list locks, calculating average weight locks, etc.\n",
    "\n",
    "# Define Flask’s routing function:\n",
    "\n",
    "/data: used to receive data from the client, calculate the global weight, and return the global weight.\n",
    "/shutdown: Used to shut down the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc750b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server(NUM_OF_CLIENTS:int):\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import threading\n",
    "    import time\n",
    "    import tensorflow as tf\n",
    "    from flask import Flask, jsonify,request\n",
    "    import os\n",
    "    \n",
    "    app = Flask(__name__)\n",
    "    clients_local_count = []\n",
    "    scaled_local_weight_list = []\n",
    "    global_value = { #Share variable\n",
    "                    'last_run_state' : False, #last run finish or not\n",
    "                    'data_state' : None,      #global_count finish or not\n",
    "                    'global_count' : None,\n",
    "                    'scale_state' : None,\n",
    "                    'weight_state' : None,\n",
    "                    'average_weights' : None,\n",
    "                    'shutdown' : 0}\n",
    "    \n",
    "    \n",
    "    \n",
    "    init_lock = threading.Lock()\n",
    "    clients_local_count_lock = threading.Lock()\n",
    "    scaled_local_weight_list_lock = threading.Lock()\n",
    "    cal_weight_lock = threading.Lock()\n",
    "    shutdown_lock = threading.Lock()\n",
    "    \n",
    "    @app.before_request\n",
    "    def before_request():\n",
    "        print('get request')\n",
    "        \n",
    "        \n",
    "    @app.route('/data', methods=['POST'])\n",
    "    def flask_server():\n",
    "        with init_lock:  #check last run is finish and init varible\n",
    "            \n",
    "            while True:\n",
    "                \n",
    "                if(len(clients_local_count)==0 and global_value['last_run_state'] == False):#init the variable by first client enter\n",
    "                    global_value['last_run_state'] = True\n",
    "                    global_value['data_state'] = False\n",
    "                    global_value['scale_state'] = False\n",
    "                    global_value['weight_state'] = False\n",
    "                    break\n",
    "                \n",
    "                elif(global_value['last_run_state'] == True):\n",
    "                    break\n",
    "                time.sleep(3)\n",
    "        \n",
    "        local_count = int(request.form.get('local_count'))          #get data\n",
    "        bs = int(request.form.get('bs'))\n",
    "        local_weight = json.loads(request.form.get('local_weight'))\n",
    "        local_weight = [np.array(lst) for lst in local_weight]\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        def scale_model_weights(weight, scalar):\n",
    "            weight_final = []\n",
    "            steps = len(weight)\n",
    "            for i in range(steps):\n",
    "                weight_final.append(scalar * weight[i])\n",
    "            return weight_final\n",
    "        def sum_scaled_weights(scaled_weight_list):\n",
    "            \n",
    "            avg_grad = list()\n",
    "            #get the average grad accross all client gradients\n",
    "            for grad_list_tuple in zip(*scaled_weight_list):\n",
    "                layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "                avg_grad.append(layer_mean)\n",
    "\n",
    "            return avg_grad\n",
    "        \n",
    "        with clients_local_count_lock:\n",
    "            clients_local_count.append(int(local_count))\n",
    "            \n",
    "        with scaled_local_weight_list_lock:\n",
    "            while True:\n",
    "                \n",
    "                if (len(clients_local_count) == NUM_OF_CLIENTS and global_value['data_state'] != True):\n",
    "                    global_value['last_run_state'] = False\n",
    "                    sum_of_local_count=sum(clients_local_count)\n",
    "                    \n",
    "                    \n",
    "                    global_value['global_count'] = sum_of_local_count     \n",
    "                    \n",
    "                    scaling_factor=local_count/global_value['global_count']\n",
    "                    scaled_weights = scale_model_weights(local_weight, scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "                    \n",
    "                    global_value['scale_state'] = True \n",
    "                    global_value['data_state'] = True\n",
    "                    break\n",
    "                elif (global_value['data_state'] == True and global_value['scale_state'] == True):\n",
    "                    scaling_factor=local_count/global_value['global_count']\n",
    "                    scaled_weights =scale_model_weights(local_weight, scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "                    break\n",
    "                time.sleep(1)\n",
    "               \n",
    "        with cal_weight_lock:\n",
    "            \n",
    "            while True:\n",
    "                if(len(scaled_local_weight_list) == NUM_OF_CLIENTS and global_value['weight_state'] != True):\n",
    "                    \n",
    "                    global_value['average_weights'] = sum_scaled_weights(scaled_local_weight_list)\n",
    "                    global_value['weight_state'] = True\n",
    "                    global_value['average_weights'] = json.dumps([np.array(w).tolist() for w in global_value['average_weights']])\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "                elif(global_value['weight_state'] == True):\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "                time.sleep(1)\n",
    "                \n",
    "                \n",
    "        clients_local_count.clear()\n",
    "        scaled_local_weight_list.clear()\n",
    "        \n",
    "        return jsonify({'result': (global_value['average_weights'])})\n",
    "        \n",
    "    \n",
    "    @app.route('/shutdown', methods=['GET'])\n",
    "    def shutdown_server():\n",
    "        global_value['shutdown'] +=1 \n",
    "        with shutdown_lock:\n",
    "            while True:\n",
    "                if(global_value['shutdown'] == NUM_OF_CLIENTS):\n",
    "                    os._exit(0)\n",
    "                    return 'Server shutting down...'\n",
    "                time.sleep(1)\n",
    "    \n",
    "    \n",
    "    app.run(host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa5d11",
   "metadata": {},
   "source": [
    "# Change the server function to container function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_op=func_to_container_op(server,base_image='tensorflow/tensorflow',packages_to_install=['flask','pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d819cb3",
   "metadata": {},
   "source": [
    "# Download data\n",
    "In this example, we use data with two labels(normal and abnormal) and download them from cloud. You can use your own data or the hemodialysis information set we provide\n",
    "\n",
    "Then generate labels for normal and abnormal data respectively, with normal labels being [1, 0] and abnormal labels being [0, 1], and merge the data and labels into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e11a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(log_folder:str,num_of_clients:int)-> NamedTuple('Outputs', [('data', str), ('label', str)]):\n",
    "    from typing import NamedTuple\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    \n",
    "    normal_url='<change yourself>' \n",
    "    abnormal_url='<change yourself>'\n",
    "\n",
    "    normal_data = pd.read_csv(normal_url)\n",
    "    abnormal_data = pd.read_csv(abnormal_url)\n",
    "\n",
    "    num_features = len(normal_data.columns)\n",
    "    print(num_features)\n",
    "\n",
    "    normal_label = np.array([[1, 0]] * len(normal_data))\n",
    "    abnormal_label = np.array([[0, 1]] * len(abnormal_data))\n",
    "\n",
    "    data = np.vstack((normal_data, abnormal_data))\n",
    "    data_label = np.vstack((normal_label, abnormal_label))\n",
    "\n",
    "    shuffler = np.random.permutation(len(data))\n",
    "    data = data[shuffler]\n",
    "    data_label = data_label[shuffler]\n",
    "\n",
    "    data = data.reshape(len(data), num_features, 1)\n",
    "    label = data_label.reshape(len(data_label), 2)\n",
    "\n",
    "    np.save(os.path.join(log_folder, 'data.npy'), data)\n",
    "    np.save(os.path.join(log_folder, 'label.npy'), label)\n",
    "    result = NamedTuple('Outputs', [('data', str), ('label', str)])\n",
    "    \n",
    "    return result(os.path.join(log_folder, 'data.npy'),\n",
    "                    os.path.join(log_folder, 'label.npy'),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3be9ff6",
   "metadata": {},
   "source": [
    "# Change the download function to container function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data_op = func_to_container_op(download_data,packages_to_install=['pandas','numpy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e89653",
   "metadata": {},
   "source": [
    "# This function is mainly used on the client to train the model and update local weights to the server.\n",
    "\n",
    "We use \"request\" for sending an HTTP request to the server.\n",
    "Actually it's not a correct federated learning code because each client should have its own data. This code is for practice only.\n",
    "\n",
    "If necessary, you can adjust the model in the \"class SimpleMLP()\".\n",
    "\n",
    "Every time the client completes training n times (you can change n in epochs), it will upload the weights to the server and wait for the adjusted weights to be sent back for the next training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client(log_folder:str,data_path: str, \n",
    "                label_path: str, batch:int,num_of_clients:int) -> NamedTuple('Outputs', [(\"last_accuracy\",float)]):\n",
    "    import json\n",
    "    import requests\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Conv1D\n",
    "    from tensorflow.keras.layers import MaxPooling1D\n",
    "    from tensorflow.keras.layers import Activation\n",
    "    from tensorflow.keras.layers import Flatten\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.optimizers import SGD\n",
    "    from tensorflow.keras import backend as K\n",
    "    \n",
    "    train_data=np.load(data_path)\n",
    "    train_label=np.load(label_path)\n",
    "    print(train_data.shape)\n",
    "    print(train_label.shape)\n",
    "    def split_and_get_batch(data, labels, x, batch_index):\n",
    "        \n",
    "        batch_size = len(data) // x\n",
    "\n",
    "        \n",
    "        data_batches = np.array_split(data, x)\n",
    "        label_batches = np.array_split(labels, x)\n",
    "\n",
    "        \n",
    "        selected_data_batch = data_batches[batch_index]\n",
    "        selected_label_batch = label_batches[batch_index]\n",
    "\n",
    "        return selected_data_batch, selected_label_batch\n",
    "    \n",
    "    data,label = split_and_get_batch(train_data,train_label,num_of_clients,batch-1)\n",
    "    print(data.shape)\n",
    "    print(label.shape)\n",
    "    full_data = list(zip(data,label))\n",
    "    class SimpleMLP:\n",
    "        @staticmethod\n",
    "        def build(shape, classes):\n",
    "            model = Sequential()\n",
    "            model.add(Conv1D(filters=4, kernel_size=3, input_shape=(17,1)))\n",
    "            model.add(MaxPooling1D(3))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(8, activation=\"relu\"))\n",
    "            model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "            return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('data len= ',len(full_data))\n",
    "    def batch_data(data_shard, bs=32):\n",
    "    \n",
    "        #seperate shard into data and labels lists\n",
    "        data, label = zip(*data_shard)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "        return dataset.shuffle(len(label)).batch(bs)\n",
    "    \n",
    "    dataset=batch_data(full_data)\n",
    "    #print(dataset)\n",
    "    \n",
    "    bs = next(iter(dataset))[0].shape[0]\n",
    "    local_count = tf.data.experimental.cardinality(dataset).numpy()*bs\n",
    "    \n",
    "    \n",
    "    loss='categorical_crossentropy'\n",
    "    metrics = ['accuracy']\n",
    "    optimizer = 'adam'\n",
    "    \n",
    "    \n",
    "    smlp_model = SimpleMLP()\n",
    "    \n",
    "    server_url=\"http://http-service:5000/data\"\n",
    "    for comm_round in range(5):\n",
    "        print('The ',comm_round+1, 'round')\n",
    "        client_model = smlp_model.build(17, 1)\n",
    "        client_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        if(comm_round == 0):\n",
    "            history = client_model.fit(dataset, epochs=1, verbose=1)\n",
    "        else:\n",
    "            client_model.set_weights(avg_weight)\n",
    "            history = client_model.fit(dataset, epochs=1, verbose=1)\n",
    "        \n",
    "        local_weight = client_model.get_weights()\n",
    "        local_weight = [np.array(w).tolist() for w in local_weight]\n",
    "        \n",
    "        client_data = {\"local_count\": local_count,'bs': bs, 'local_weight': json.dumps(local_weight)}\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                weight = (requests.post(server_url,data=client_data))\n",
    "                \n",
    "                if weight.status_code == 200:\n",
    "                    print(f\"exist\")\n",
    "\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"server error\")\n",
    "\n",
    "            except requests.exceptions.RequestException:\n",
    "\n",
    "                print(f\"not exist\")\n",
    "                \n",
    "            time.sleep(5)\n",
    "            \n",
    "        data = weight.json()\n",
    "        avg_weight = data.get('result')\n",
    "        avg_weight = json.loads(avg_weight)\n",
    "        avg_weight = [np.array(lst) for lst in avg_weight]\n",
    "        \n",
    "    shutdown_url=\"http://http-service:5000/shutdown\"    \n",
    "    try:\n",
    "        response = requests.get(shutdown_url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print('already shutdown')\n",
    "    last_accuracy = history.history['accuracy'][-1]\n",
    "    print(last_accuracy)\n",
    "    return([last_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782dbadb",
   "metadata": {},
   "source": [
    "# Change the client function to container function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed8fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_op=func_to_container_op(client,base_image='tensorflow/tensorflow',packages_to_install=['requests','pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de3240",
   "metadata": {},
   "source": [
    "# Receive and convert the test accuracy results into string format for subsequent processing or display.\n",
    "\n",
    "Convert the test accuracy results into string format and separate each value with commas.\n",
    "Remove the last comma to avoid extra delimiters.\n",
    "\n",
    "Then change the show_result function to container function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d68361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(test_acc: List[float]) -> NamedTuple('Outputs', [('test_accuracy', str)]):\n",
    "    \n",
    "    result_str = ', '.join(map(str, test_acc))\n",
    "    print(result_str)\n",
    "    \n",
    "    result_str = result_str[:-1]\n",
    "    \n",
    "    return (result_str,)\n",
    "\n",
    "show_results_op = func_to_container_op(show_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1197be7",
   "metadata": {},
   "source": [
    "# Generate custom number of clients\n",
    "\n",
    "This function will generate the containers(task) and connect them together(download_data_task, client_task and show_results_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clients(log_folder,n):# generate the num of clients\n",
    "    global vop\n",
    "    client_tasks = []\n",
    "    download_data_task = download_data_op(log_folder,n)\n",
    "    download_data_task.set_cpu_request('0.1').set_cpu_limit('0.1').add_pvolumes({\n",
    "        log_folder:vop.volume,\n",
    "    })\n",
    "    for i in range(n):\n",
    "        client_task = globals()['client_task_' + str(i + 1)] = client_op(log_folder, download_data_task.outputs['data'],\n",
    "                                        download_data_task.outputs['label'],i + 1,n)\n",
    "        client_task.set_cpu_request('0.1').set_cpu_limit('0.1').add_pvolumes({\n",
    "        log_folder:vop.volume,\n",
    "    })\n",
    "        client_tasks.append(client_task)\n",
    "    \n",
    "    globals()['show_results_task'] = show_results_op([ct.outputs['last_accuracy'] for ct in client_tasks]).set_cpu_request('0.1').set_cpu_limit('0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e94526",
   "metadata": {},
   "source": [
    "# Define the pipeline\n",
    "\n",
    "The first half part we define the path(log_folder) for saving the data and create a volume.\n",
    "Then the server part we use \"Service\" to let client containers can connect to the server container. After the server shutdown, this service will be deleted.\n",
    "\n",
    "The second half we use generate_clients() to generate the other container, include download_data, clients and show result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='FL test'\n",
    "    )\n",
    "def fl_pipeline(namespace='kubeflow-user-thu01'):\n",
    "    global NUM_OF_CLIENTS\n",
    "    log_folder = '/data'\n",
    "    global vop\n",
    "    pvc_name = \"mypvc\"\n",
    "    vop = dsl.VolumeOp(\n",
    "        name=pvc_name,\n",
    "        resource_name=\"newpvc\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO\n",
    "    )\n",
    "    \n",
    "    \n",
    "    service = dsl.ResourceOp(\n",
    "        name='http-service',\n",
    "        k8s_resource={\n",
    "            'apiVersion': 'v1',\n",
    "            'kind': 'Service',\n",
    "            'metadata': {\n",
    "                'name': 'http-service'\n",
    "            },\n",
    "            'spec': {\n",
    "                'selector': {\n",
    "                    'app': 'http-service'\n",
    "                },\n",
    "                'ports': [\n",
    "                    {\n",
    "                        'protocol': 'TCP',\n",
    "                        'port': 5000,\n",
    "                        'targetPort': 8080\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    )#service_end\n",
    "    server_task=server_op(NUM_OF_CLIENTS)\n",
    "    server_task.add_pod_label('app', 'http-service')\n",
    "    server_task.add_port(V1ContainerPort(name='my-port', container_port=8080))\n",
    "    server_task.set_cpu_request('0.1').set_cpu_limit('0.1')\n",
    "    server_task.after(service)\n",
    "    \n",
    "    delete_service = dsl.ResourceOp(\n",
    "        name='delete-service',\n",
    "        k8s_resource={\n",
    "            'apiVersion': 'v1',\n",
    "            'kind': 'Service',\n",
    "            'metadata': {\n",
    "                'name': 'http-service'\n",
    "            },\n",
    "            'spec': {\n",
    "                'selector': {\n",
    "                    'app': 'http-service'\n",
    "                },\n",
    "                'ports': [\n",
    "                    {\n",
    "                        'protocol': 'TCP',\n",
    "                        'port': 80,\n",
    "                        'targetPort': 8080\n",
    "                    }\n",
    "                ],\n",
    "                'type': 'NodePort'  \n",
    "            }\n",
    "        },\n",
    "        action=\"delete\" #刪除\n",
    "    ).after(server_task)\n",
    "    \n",
    "    generate_clients(log_folder,NUM_OF_CLIENTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4847835",
   "metadata": {},
   "source": [
    "# Convert the pipeline into yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ccf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    kfp.compiler.Compiler().compile(fl_pipeline, 'fl_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
